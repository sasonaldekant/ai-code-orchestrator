"""\nParallel execution engine for concurrent task processing.\n\nThis module enables parallel execution of independent tasks to reduce\nend-to-end latency. For example, backend and frontend implementation\ncan run concurrently, reducing total pipeline time by ~40%.\n\nFeatures:\n- Task dependency resolution\n- Concurrent execution with asyncio.gather\n- Resource pooling and limits\n- Error handling with partial results\n- Progress tracking\n\nVersion: 2.0.0\n\"""\n\nfrom __future__ import annotations\n\nimport asyncio\nimport logging\nimport time\nfrom typing import List, Dict, Any, Optional, Callable, Set\nfrom dataclasses import dataclass, field\nfrom enum import Enum\n\nlogger = logging.getLogger(__name__)\n\n\nclass TaskStatus(Enum):\n    \"\"\"Status of a task in the execution pipeline.\"\"\"\n    PENDING = \"pending\"\n    RUNNING = \"running\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    SKIPPED = \"skipped\"\n\n\n@dataclass\nclass Task:\n    \"\"\"\n    A single task to be executed.\n    \n    Attributes\n    ----------\n    id : str\n        Unique task identifier.\n    name : str\n        Human-readable task name.\n    executor : callable\n        Async function to execute the task.\n    dependencies : list\n        Task IDs that must complete before this task.\n    priority : int\n        Execution priority (higher = sooner).\n    timeout : int\n        Maximum execution time in seconds.\n    \"\"\"\n    id: str\n    name: str\n    executor: Callable[..., Any]\n    kwargs: Dict[str, Any] = field(default_factory=dict)\n    dependencies: List[str] = field(default_factory=list)\n    priority: int = 0\n    timeout: int = 300  # 5 minutes default\n    allow_failure: bool = False\n\n\n@dataclass\nclass TaskResult:\n    \"\"\"Result of a single task execution.\"\"\"\n    task_id: str\n    task_name: str\n    status: TaskStatus\n    output: Any\n    error: Optional[str]\n    start_time: float\n    end_time: float\n    duration: float\n    tokens_used: int = 0\n    cost: float = 0.0\n\n\n@dataclass\nclass ExecutionPlan:\n    \"\"\"\n    Execution plan with resolved dependencies.\n    \n    Tasks are organized into batches where each batch contains\n    tasks that can run in parallel.\n    \"\"\"\n    batches: List[List[Task]]\n    total_tasks: int\n    max_parallelism: int\n\n\n@dataclass\nclass ExecutionResult:\n    \"\"\"Overall result of parallel execution.\"\"\"\n    task_results: Dict[str, TaskResult]\n    total_duration: float\n    successful_tasks: int\n    failed_tasks: int\n    total_tokens: int\n    total_cost: float\n    speedup_factor: float  # Compared to sequential execution\n\n\nclass ParallelExecutor:\n    \"\"\"\n    Execute tasks in parallel with dependency resolution.\n    \n    The executor:\n    1. Resolves task dependencies\n    2. Groups tasks into parallel batches\n    3. Executes each batch concurrently\n    4. Handles errors and collects results\n    5. Tracks metrics and progress\n    \n    Example\n    -------\n    >>> executor = ParallelExecutor(max_concurrent=3)\n    >>> \n    >>> async def implement_backend(**kwargs):\n    ...     # Implementation logic\n    ...     return \"Backend code\"\n    >>> \n    >>> async def implement_frontend(**kwargs):\n    ...     # Implementation logic\n    ...     return \"Frontend code\"\n    >>> \n    >>> tasks = [\n    ...     Task(\n    ...         id=\"backend\",\n    ...         name=\"Backend Implementation\",\n    ...         executor=implement_backend,\n    ...         kwargs={\"spec\": backend_spec}\n    ...     ),\n    ...     Task(\n    ...         id=\"frontend\",\n    ...         name=\"Frontend Implementation\",\n    ...         executor=implement_frontend,\n    ...         kwargs={\"spec\": frontend_spec}\n    ...     )\n    ... ]\n    >>> \n    >>> result = await executor.execute(tasks)\n    >>> print(f\"Speedup: {result.speedup_factor:.1f}x\")\n    \"\"\"\n    \n    def __init__(\n        self,\n        max_concurrent: int = 5,\n        cost_manager: Optional[Any] = None\n    ):\n        self.max_concurrent = max_concurrent\n        self.cost_manager = cost_manager\n        self._semaphore = asyncio.Semaphore(max_concurrent)\n    \n    async def execute(\n        self,\n        tasks: List[Task],\n        fail_fast: bool = False\n    ) -> ExecutionResult:\n        \"\"\"\n        Execute tasks with dependency resolution and parallelism.\n        \n        Parameters\n        ----------\n        tasks : list\n            List of Task objects to execute.\n        fail_fast : bool\n            If True, stop execution on first failure.\n        \n        Returns\n        -------\n        ExecutionResult\n            Aggregated results from all tasks.\n        \"\"\"\n        logger.info(f\"Starting parallel execution of {len(tasks)} tasks\")\n        start_time = time.time()\n        \n        # Build execution plan\n        plan = self._build_execution_plan(tasks)\n        logger.info(\n            f\"Execution plan: {len(plan.batches)} batches, \"\n            f\"max parallelism: {plan.max_parallelism}\"\n        )\n        \n        # Execute batches sequentially, tasks within batch in parallel\n        task_results = {}\n        completed_tasks = set()\n        \n        for batch_num, batch in enumerate(plan.batches, 1):\n            logger.info(f\"Executing batch {batch_num}/{len(plan.batches)} ({len(batch)} tasks)\")\n            \n            batch_results = await self._execute_batch(\n                batch=batch,\n                completed_tasks=completed_tasks,\n                all_results=task_results\n            )\n            \n            # Update results and completed set\n            task_results.update(batch_results)\n            completed_tasks.update(batch_results.keys())\n            \n            # Check for failures in fail_fast mode\n            if fail_fast:\n                failed = [\n                    r for r in batch_results.values()\n                    if r.status == TaskStatus.FAILED\n                ]\n                if failed:\n                    logger.error(f\"Fail-fast triggered: {len(failed)} tasks failed\")\n                    # Mark remaining tasks as skipped\n                    for task in tasks:\n                        if task.id not in task_results:\n                            task_results[task.id] = TaskResult(\n                                task_id=task.id,\n                                task_name=task.name,\n                                status=TaskStatus.SKIPPED,\n                                output=None,\n                                error=\"Skipped due to fail-fast\",\n                                start_time=time.time(),\n                                end_time=time.time(),\n                                duration=0.0\n                            )\n                    break\n        \n        # Aggregate results\n        total_duration = time.time() - start_time\n        \n        successful = sum(\n            1 for r in task_results.values()\n            if r.status == TaskStatus.COMPLETED\n        )\n        failed = sum(\n            1 for r in task_results.values()\n            if r.status == TaskStatus.FAILED\n        )\n        \n        total_tokens = sum(r.tokens_used for r in task_results.values())\n        total_cost = sum(r.cost for r in task_results.values())\n        \n        # Calculate speedup (sequential time vs parallel time)\n        sequential_duration = sum(r.duration for r in task_results.values())\n        speedup = sequential_duration / total_duration if total_duration > 0 else 1.0\n        \n        logger.info(\n            f\"Execution complete: {successful}/{len(tasks)} successful, \"\n            f\"speedup: {speedup:.2f}x, duration: {total_duration:.1f}s\"\n        )\n        \n        return ExecutionResult(\n            task_results=task_results,\n            total_duration=total_duration,\n            successful_tasks=successful,\n            failed_tasks=failed,\n            total_tokens=total_tokens,\n            total_cost=total_cost,\n            speedup_factor=speedup\n        )\n    \n    def _build_execution_plan(self, tasks: List[Task]) -> ExecutionPlan:\n        \"\"\"\n        Resolve dependencies and organize tasks into parallel batches.\n        \n        Uses topological sorting to ensure dependencies are satisfied.\n        \"\"\"\n        task_map = {task.id: task for task in tasks}\n        batches = []\n        remaining = set(task_map.keys())\n        completed = set()\n        \n        while remaining:\n            # Find tasks with satisfied dependencies\n            ready = [\n                task_id for task_id in remaining\n                if all(dep in completed for dep in task_map[task_id].dependencies)\n            ]\n            \n            if not ready:\n                # Circular dependency or orphaned tasks\n                logger.error(f\"Cannot resolve dependencies for: {remaining}\")\n                # Add them anyway to avoid infinite loop\n                ready = list(remaining)\n            \n            # Sort by priority\n            ready.sort(key=lambda tid: task_map[tid].priority, reverse=True)\n            \n            batch = [task_map[tid] for tid in ready]\n            batches.append(batch)\n            \n            remaining -= set(ready)\n            completed.update(ready)\n        \n        max_parallelism = max(len(batch) for batch in batches) if batches else 0\n        \n        return ExecutionPlan(\n            batches=batches,\n            total_tasks=len(tasks),\n            max_parallelism=max_parallelism\n        )\n    \n    async def _execute_batch(\n        self,\n        batch: List[Task],\n        completed_tasks: Set[str],\n        all_results: Dict[str, TaskResult]\n    ) -> Dict[str, TaskResult]:\n        \"\"\"Execute all tasks in a batch concurrently.\"\"\"\n        # Create coroutines for each task\n        coroutines = [\n            self._execute_task(task, all_results)\n            for task in batch\n        ]\n        \n        # Execute in parallel\n        results = await asyncio.gather(*coroutines, return_exceptions=True)\n        \n        # Map results back to task IDs\n        batch_results = {}\n        for task, result in zip(batch, results):\n            if isinstance(result, Exception):\n                # Execution raised an exception\n                batch_results[task.id] = TaskResult(\n                    task_id=task.id,\n                    task_name=task.name,\n                    status=TaskStatus.FAILED,\n                    output=None,\n                    error=str(result),\n                    start_time=time.time(),\n                    end_time=time.time(),\n                    duration=0.0\n                )\n            else:\n                batch_results[task.id] = result\n        \n        return batch_results\n    \n    async def _execute_task(\n        self,\n        task: Task,\n        all_results: Dict[str, TaskResult]\n    ) -> TaskResult:\n        \"\"\"Execute a single task with timeout and error handling.\"\"\"\n        logger.info(f\"Starting task: {task.name} (ID: {task.id})\")\n        start_time = time.time()\n        \n        # Acquire semaphore to limit concurrency\n        async with self._semaphore:\n            try:\n                # Check cost budget if cost manager available\n                if self.cost_manager and not self.cost_manager.can_proceed():\n                    return TaskResult(\n                        task_id=task.id,\n                        task_name=task.name,\n                        status=TaskStatus.FAILED,\n                        output=None,\n                        error=\"Cost budget exceeded\",\n                        start_time=start_time,\n                        end_time=time.time(),\n                        duration=time.time() - start_time\n                    )\n                \n                # Inject dependency results into kwargs\n                task_kwargs = task.kwargs.copy()\n                for dep_id in task.dependencies:\n                    if dep_id in all_results:\n                        task_kwargs[f\"{dep_id}_result\"] = all_results[dep_id].output\n                \n                # Execute with timeout\n                output = await asyncio.wait_for(\n                    task.executor(**task_kwargs),\n                    timeout=task.timeout\n                )\n                \n                end_time = time.time()\n                duration = end_time - start_time\n                \n                # Extract tokens/cost if available\n                tokens_used = 0\n                cost = 0.0\n                if isinstance(output, dict):\n                    tokens_used = output.get(\"tokens_used\", 0)\n                    cost = output.get(\"cost\", 0.0)\n                \n                logger.info(\n                    f\"Task completed: {task.name} in {duration:.1f}s \"\n                    f\"({tokens_used} tokens, ${cost:.4f})\"\n                )\n                \n                return TaskResult(\n                    task_id=task.id,\n                    task_name=task.name,\n                    status=TaskStatus.COMPLETED,\n                    output=output,\n                    error=None,\n                    start_time=start_time,\n                    end_time=end_time,\n                    duration=duration,\n                    tokens_used=tokens_used,\n                    cost=cost\n                )\n            \n            except asyncio.TimeoutError:\n                logger.error(f\"Task timed out: {task.name} (>{task.timeout}s)\")\n                return TaskResult(\n                    task_id=task.id,\n                    task_name=task.name,\n                    status=TaskStatus.FAILED,\n                    output=None,\n                    error=f\"Timeout after {task.timeout}s\",\n                    start_time=start_time,\n                    end_time=time.time(),\n                    duration=time.time() - start_time\n                )\n            \n            except Exception as e:\n                logger.error(f\"Task failed: {task.name} - {e}\", exc_info=True)\n                \n                if task.allow_failure:\n                    logger.warning(f\"Task failure allowed: {task.name}\")\n                \n                return TaskResult(\n                    task_id=task.id,\n                    task_name=task.name,\n                    status=TaskStatus.FAILED,\n                    output=None,\n                    error=str(e),\n                    start_time=start_time,\n                    end_time=time.time(),\n                    duration=time.time() - start_time\n                )\n    \n    def get_metrics(self, result: ExecutionResult) -> Dict[str, Any]:\n        \"\"\"Extract metrics from execution result.\"\"\"\n        return {\n            \"total_tasks\": len(result.task_results),\n            \"successful_tasks\": result.successful_tasks,\n            \"failed_tasks\": result.failed_tasks,\n            \"success_rate\": result.successful_tasks / len(result.task_results) if result.task_results else 0,\n            \"total_duration_seconds\": result.total_duration,\n            \"speedup_factor\": result.speedup_factor,\n            \"total_tokens\": result.total_tokens,\n            \"total_cost_usd\": result.total_cost,\n            \"avg_task_duration\": sum(\n                r.duration for r in result.task_results.values()\n            ) / len(result.task_results) if result.task_results else 0\n        }\n