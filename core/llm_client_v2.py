"""\nEnhanced LLM client supporting multiple providers (OpenAI, Anthropic, Google).\n\nThis module provides a unified interface for interacting with different LLM APIs\nwhile handling provider-specific authentication, request formatting, and response parsing.\n\nVersion: 2.0.0\n\"""\n\nfrom __future__ import annotations\n\nimport os\nimport asyncio\nimport logging\nfrom typing import Dict, Any, Optional, List\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass LLMResponse:\n    \"\"\"Standardized response from any LLM provider.\"\"\"\n    content: str\n    model: str\n    provider: str\n    tokens_used: Dict[str, int]  # {\"prompt\": int, \"completion\": int, \"total\": int}\n    finish_reason: str\n    metadata: Dict[str, Any]\n\n\nclass LLMProvider(ABC):\n    \"\"\"Abstract base class for LLM providers.\"\"\"\n    \n    def __init__(self, api_key: str, cost_manager: Any):\n        self.api_key = api_key\n        self.cost_manager = cost_manager\n    \n    @abstractmethod\n    async def complete(\n        self,\n        messages: List[Dict[str, str]],\n        model: str,\n        temperature: float = 0.0,\n        max_tokens: int = 4000,\n        **kwargs\n    ) -> LLMResponse:\n        \"\"\"Generate completion from the LLM.\"\"\"\n        pass\n\n\nclass OpenAIProvider(LLMProvider):\n    \"\"\"OpenAI API provider (GPT-4o, GPT-4o-mini).\"\"\"\n    \n    async def complete(\n        self,\n        messages: List[Dict[str, str]],\n        model: str,\n        temperature: float = 0.0,\n        max_tokens: int = 4000,\n        **kwargs\n    ) -> LLMResponse:\n        \"\"\"Call OpenAI Chat Completions API.\"\"\"\n        try:\n            from openai import AsyncOpenAI\n            \n            client = AsyncOpenAI(api_key=self.api_key)\n            \n            response = await client.chat.completions.create(\n                model=model,\n                messages=messages,\n                temperature=temperature,\n                max_tokens=max_tokens,\n                **kwargs\n            )\n            \n            usage = response.usage\n            tokens_used = {\n                \"prompt\": usage.prompt_tokens,\n                \"completion\": usage.completion_tokens,\n                \"total\": usage.total_tokens\n            }\n            \n            # Track cost\n            self.cost_manager.check_and_update(\n                model=model,\n                tokens_in=tokens_used[\"prompt\"],\n                tokens_out=tokens_used[\"completion\"]\n            )\n            \n            return LLMResponse(\n                content=response.choices[0].message.content,\n                model=model,\n                provider=\"openai\",\n                tokens_used=tokens_used,\n                finish_reason=response.choices[0].finish_reason,\n                metadata={\"id\": response.id}\n            )\n            \n        except Exception as e:\n            logger.error(f\"OpenAI API error: {e}\")\n            raise\n\n\nclass AnthropicProvider(LLMProvider):\n    \"\"\"Anthropic API provider (Claude 3.5 Sonnet).\"\"\"\n    \n    async def complete(\n        self,\n        messages: List[Dict[str, str]],\n        model: str,\n        temperature: float = 0.0,\n        max_tokens: int = 4000,\n        **kwargs\n    ) -> LLMResponse:\n        \"\"\"Call Anthropic Messages API.\"\"\"\n        try:\n            from anthropic import AsyncAnthropic\n            \n            client = AsyncAnthropic(api_key=self.api_key)\n            \n            # Convert OpenAI-style messages to Anthropic format\n            system_messages = [m[\"content\"] for m in messages if m[\"role\"] == \"system\"]\n            user_messages = [m for m in messages if m[\"role\"] != \"system\"]\n            \n            response = await client.messages.create(\n                model=model,\n                system=system_messages[0] if system_messages else \"\",\n                messages=user_messages,\n                temperature=temperature,\n                max_tokens=max_tokens,\n                **kwargs\n            )\n            \n            tokens_used = {\n                \"prompt\": response.usage.input_tokens,\n                \"completion\": response.usage.output_tokens,\n                \"total\": response.usage.input_tokens + response.usage.output_tokens\n            }\n            \n            # Track cost\n            self.cost_manager.check_and_update(\n                model=model,\n                tokens_in=tokens_used[\"prompt\"],\n                tokens_out=tokens_used[\"completion\"]\n            )\n            \n            return LLMResponse(\n                content=response.content[0].text,\n                model=model,\n                provider=\"anthropic\",\n                tokens_used=tokens_used,\n                finish_reason=response.stop_reason,\n                metadata={\"id\": response.id}\n            )\n            \n        except Exception as e:\n            logger.error(f\"Anthropic API error: {e}\")\n            raise\n\n\nclass GoogleProvider(LLMProvider):\n    \"\"\"Google Generative AI provider (Gemini 2.5 Pro).\"\"\"\n    \n    async def complete(\n        self,\n        messages: List[Dict[str, str]],\n        model: str,\n        temperature: float = 0.0,\n        max_tokens: int = 4000,\n        **kwargs\n    ) -> LLMResponse:\n        \"\"\"Call Google Generative AI API.\"\"\"\n        try:\n            import google.generativeai as genai\n            \n            genai.configure(api_key=self.api_key)\n            \n            # Convert to Gemini format\n            gemini_model = genai.GenerativeModel(model)\n            \n            # Merge messages into conversation format\n            conversation = \"\\n\\n\".join([\n                f\"{m['role'].upper()}: {m['content']}\" for m in messages\n            ])\n            \n            response = await gemini_model.generate_content_async(\n                conversation,\n                generation_config={\n                    \"temperature\": temperature,\n                    \"max_output_tokens\": max_tokens,\n                }\n            )\n            \n            # Gemini doesn't always provide detailed token counts\n            # Estimate based on content length (rough approximation)\n            prompt_tokens = sum(len(m[\"content\"].split()) for m in messages) * 1.3\n            completion_tokens = len(response.text.split()) * 1.3\n            \n            tokens_used = {\n                \"prompt\": int(prompt_tokens),\n                \"completion\": int(completion_tokens),\n                \"total\": int(prompt_tokens + completion_tokens)\n            }\n            \n            # Track cost\n            self.cost_manager.check_and_update(\n                model=model,\n                tokens_in=tokens_used[\"prompt\"],\n                tokens_out=tokens_used[\"completion\"]\n            )\n            \n            return LLMResponse(\n                content=response.text,\n                model=model,\n                provider=\"google\",\n                tokens_used=tokens_used,\n                finish_reason=\"stop\",\n                metadata={}\n            )\n            \n        except Exception as e:\n            logger.error(f\"Google API error: {e}\")\n            raise\n\n\nclass LLMClient:\n    \"\"\"\n    Unified client for multi-provider LLM access.\n    \n    Automatically routes requests to the appropriate provider based on model name.\n    Handles authentication, rate limiting, retries, and cost tracking.\n    \n    Example\n    -------\n    >>> from core.cost_manager import CostManager\n    >>> cost_manager = CostManager()\n    >>> client = LLMClient(cost_manager)\n    >>> \n    >>> messages = [\n    ...     {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    ...     {\"role\": \"user\", \"content\": \"Explain async/await in Python.\"}\n    ... ]\n    >>> \n    >>> response = await client.complete(\n    ...     messages=messages,\n    ...     model=\"gpt-4o-mini\",\n    ...     temperature=0.0\n    ... )\n    >>> print(response.content)\n    \"\"\"\n    \n    def __init__(self, cost_manager: Any):\n        self.cost_manager = cost_manager\n        self.providers: Dict[str, LLMProvider] = {}\n        \n        # Initialize providers based on available API keys\n        if os.getenv(\"OPENAI_API_KEY\"):\n            self.providers[\"openai\"] = OpenAIProvider(\n                os.getenv(\"OPENAI_API_KEY\"), cost_manager\n            )\n        \n        if os.getenv(\"ANTHROPIC_API_KEY\"):\n            self.providers[\"anthropic\"] = AnthropicProvider(\n                os.getenv(\"ANTHROPIC_API_KEY\"), cost_manager\n            )\n        \n        if os.getenv(\"GOOGLE_API_KEY\"):\n            self.providers[\"google\"] = GoogleProvider(\n                os.getenv(\"GOOGLE_API_KEY\"), cost_manager\n            )\n        \n        logger.info(f\"Initialized LLM client with providers: {list(self.providers.keys())}\")\n    \n    def _get_provider_for_model(self, model: str) -> str:\n        \"\"\"Determine provider based on model name.\"\"\"\n        model_lower = model.lower()\n        if \"gpt\" in model_lower:\n            return \"openai\"\n        elif \"claude\" in model_lower:\n            return \"anthropic\"\n        elif \"gemini\" in model_lower:\n            return \"google\"\n        else:\n            raise ValueError(f\"Unknown model provider for: {model}\")\n    \n    async def complete(\n        self,\n        messages: List[Dict[str, str]],\n        model: str,\n        temperature: float = 0.0,\n        max_tokens: int = 4000,\n        **kwargs\n    ) -> LLMResponse:\n        \"\"\"\n        Generate completion using the specified model.\n        \n        Parameters\n        ----------\n        messages : list\n            List of message dicts with \"role\" and \"content\" keys.\n        model : str\n            Model identifier (e.g., \"gpt-4o\", \"claude-3-5-sonnet\", \"gemini-2.5-pro\").\n        temperature : float\n            Sampling temperature (0.0 = deterministic).\n        max_tokens : int\n            Maximum tokens to generate.\n        \n        Returns\n        -------\n        LLMResponse\n            Standardized response object.\n        \"\"\"\n        provider_name = self._get_provider_for_model(model)\n        \n        if provider_name not in self.providers:\n            raise ValueError(\n                f\"Provider {provider_name} not available. \"\n                f\"Please set the appropriate API key.\"\n            )\n        \n        provider = self.providers[provider_name]\n        \n        logger.info(f\"Calling {provider_name} with model {model}\")\n        \n        # Check budget before calling\n        can_proceed, _ = self.cost_manager.check_and_update(model, 0, 0)\n        if not can_proceed:\n            raise RuntimeError(\"Cost budget exceeded. Cannot proceed with LLM call.\")\n        \n        return await provider.complete(\n            messages=messages,\n            model=model,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            **kwargs\n        )\n