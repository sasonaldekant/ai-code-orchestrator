"""\nModel router for intelligent LLM selection based on task characteristics.\n\nReads configuration from YAML and routes requests to the optimal model\nconsidering reasoning requirements, cost constraints, context limits,\nand consensus needs.\n\nVersion: 2.0.0\n\"""\n\nfrom __future__ import annotations\n\nimport yaml\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional, List\nfrom dataclasses import dataclass, field\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass ModelConfig:\n    \"\"\"Configuration for a specific model.\"\"\"\n    model: str\n    provider: str\n    temperature: float\n    max_tokens: int\n    reasoning: Optional[str] = None\n    consensus_mode: bool = False\n    producer_reviewer_loop: bool = False\n    max_iterations: int = 1\n    quality_threshold: float = 8.0\n    context_aware: bool = False\n    context_sources: List[str] = field(default_factory=list)\n    specialization: Optional[str] = None\n\n\n@dataclass\nclass ConsensusConfig:\n    \"\"\"Configuration for consensus mode with multiple models.\"\"\"\n    primary: ModelConfig\n    secondary: ModelConfig\n    tertiary: Optional[ModelConfig] = None\n    synthesis_model: str = \"claude-3-5-sonnet\"\n    weight_primary: float = 0.5\n    weight_secondary: float = 0.3\n    weight_tertiary: float = 0.2\n    reasoning: Optional[str] = None\n\n\nclass ModelRouter:\n    \"\"\"\n    Route LLM requests to optimal models based on task characteristics.\n    \n    The router supports:\n    - Phase-based routing (analyst, architect, implementer, tester, reviewer)\n    - Specialty routing (backend, frontend, documentation, etc.)\n    - Consensus mode (multiple models vote)\n    - Producer-reviewer loops\n    \n    Example\n    -------\n    >>> router = ModelRouter(\"config/model_mapping_v2.yaml\")\n    >>> config = router.get_model_for_phase(\"analyst\")\n    >>> print(config.model)  # claude-3-5-sonnet\n    >>> print(config.reasoning)  # Best for requirements analysis...\n    \"\"\"\n    \n    def __init__(self, config_path: str = \"config/model_mapping_v2.yaml\"):\n        self.config_path = Path(config_path)\n        self.config = self._load_config()\n        logger.info(f\"Model router initialized from {config_path}\")\n    \n    def _load_config(self) -> Dict[str, Any]:\n        \"\"\"Load and parse YAML configuration.\"\"\"\n        if not self.config_path.exists():\n            logger.warning(f\"Config file {self.config_path} not found, using defaults\")\n            return self._get_default_config()\n        \n        with open(self.config_path, \"r\") as f:\n            return yaml.safe_load(f)\n    \n    def _get_default_config(self) -> Dict[str, Any]:\n        \"\"\"Return default configuration if YAML not found.\"\"\"\n        return {\n            \"default\": {\n                \"model\": \"gpt-4o-mini\",\n                \"temperature\": 0.0,\n                \"max_tokens\": 4000\n            }\n        }\n    \n    def get_model_for_phase(self, phase: str) -> ModelConfig:\n        \"\"\"\n        Get model configuration for a specific phase.\n        \n        Parameters\n        ----------\n        phase : str\n            Phase name (e.g., \"analyst\", \"architect\", \"implementer\").\n        \n        Returns\n        -------\n        ModelConfig\n            Configuration for the selected model.\n        \n        Examples\n        --------\n        >>> router = ModelRouter()\n        >>> config = router.get_model_for_phase(\"implementer\")\n        >>> print(f\"{config.model} - {config.reasoning}\")\n        gpt-4o - Strong C# performance (92% HumanEval)\n        \"\"\"\n        phase_config = self.config.get(\"routing\", {}).get(\"phase\", {}).get(phase)\n        \n        if not phase_config:\n            # Fall back to default\n            default = self.config.get(\"default\", {})\n            return ModelConfig(\n                model=default.get(\"model\", \"gpt-4o-mini\"),\n                provider=\"openai\",\n                temperature=default.get(\"temperature\", 0.0),\n                max_tokens=default.get(\"max_tokens\", 4000)\n            )\n        \n        # Check for consensus mode\n        if phase_config.get(\"consensus_mode\"):\n            # Return primary model but flag consensus mode\n            primary = phase_config[\"models\"][\"primary\"]\n            return ModelConfig(\n                model=primary[\"model\"],\n                provider=primary[\"provider\"],\n                temperature=0.1,\n                max_tokens=8000,\n                consensus_mode=True,\n                reasoning=phase_config.get(\"reasoning\", \"\")\n            )\n        \n        # Standard single-model routing\n        return ModelConfig(\n            model=phase_config[\"model\"],\n            provider=phase_config[\"provider\"],\n            temperature=phase_config.get(\"temperature\", 0.0),\n            max_tokens=phase_config.get(\"max_tokens\", 4000),\n            producer_reviewer_loop=phase_config.get(\"producer_reviewer_loop\", False),\n            max_iterations=phase_config.get(\"max_iterations\", 1),\n            quality_threshold=phase_config.get(\"quality_threshold\", 8.0),\n            reasoning=phase_config.get(\"reasoning\", \"\")\n        )\n    \n    def get_model_for_specialty(\n        self,\n        category: str,\n        specialty: str\n    ) -> ModelConfig:\n        \"\"\"\n        Get model for a specialty task.\n        \n        Parameters\n        ----------\n        category : str\n            Category (e.g., \"backend\", \"frontend\", \"integration\").\n        specialty : str\n            Specific specialty (e.g., \"dotnet_api\", \"react\", \"efcore\").\n        \n        Returns\n        -------\n        ModelConfig\n            Configuration for the selected model.\n        \n        Examples\n        --------\n        >>> router = ModelRouter()\n        >>> config = router.get_model_for_specialty(\"backend\", \"efcore\")\n        >>> print(config.context_aware)  # True\n        >>> print(config.context_sources)  # ['existing_dbcontext', 'migration_history']\n        \"\"\"\n        specialty_config = (\n            self.config.get(\"routing\", {})\n            .get(\"specialty\", {})\n            .get(category, {})\n            .get(specialty)\n        )\n        \n        if not specialty_config:\n            raise ValueError(f\"No configuration found for {category}/{specialty}\")\n        \n        return ModelConfig(\n            model=specialty_config[\"model\"],\n            provider=specialty_config[\"provider\"],\n            temperature=specialty_config.get(\"temperature\", 0.0),\n            max_tokens=specialty_config.get(\"max_tokens\", 4000),\n            context_aware=specialty_config.get(\"context_aware\", False),\n            context_sources=specialty_config.get(\"context_sources\", []),\n            specialization=specialty_config.get(\"specialization\", \"\")\n        )\n    \n    def get_consensus_models(self, phase: str) -> ConsensusConfig:\n        \"\"\"\n        Get all models for consensus mode.\n        \n        Parameters\n        ----------\n        phase : str\n            Phase name (e.g., \"architect\").\n        \n        Returns\n        -------\n        ConsensusConfig\n            Configuration containing primary, secondary, and tertiary models.\n        \n        Examples\n        --------\n        >>> router = ModelRouter()\n        >>> consensus = router.get_consensus_models(\"architect\")\n        >>> print(consensus.primary.model)  # claude-3-5-sonnet\n        >>> print(consensus.secondary.model)  # gpt-4o\n        >>> print(consensus.weight_primary)  # 0.5\n        \"\"\"\n        phase_config = self.config.get(\"routing\", {}).get(\"phase\", {}).get(phase)\n        \n        if not phase_config or not phase_config.get(\"consensus_mode\"):\n            raise ValueError(f\"Phase {phase} not configured for consensus mode\")\n        \n        models = phase_config[\"models\"]\n        \n        primary_cfg = models[\"primary\"]\n        primary = ModelConfig(\n            model=primary_cfg[\"model\"],\n            provider=primary_cfg[\"provider\"],\n            temperature=0.1,\n            max_tokens=8000\n        )\n        \n        secondary_cfg = models[\"secondary\"]\n        secondary = ModelConfig(\n            model=secondary_cfg[\"model\"],\n            provider=secondary_cfg[\"provider\"],\n            temperature=0.1,\n            max_tokens=8000\n        )\n        \n        tertiary = None\n        if \"tertiary\" in models:\n            tertiary_cfg = models[\"tertiary\"]\n            tertiary = ModelConfig(\n                model=tertiary_cfg[\"model\"],\n                provider=tertiary_cfg[\"provider\"],\n                temperature=0.1,\n                max_tokens=8000\n            )\n        \n        return ConsensusConfig(\n            primary=primary,\n            secondary=secondary,\n            tertiary=tertiary,\n            synthesis_model=phase_config.get(\"synthesis_model\", \"claude-3-5-sonnet\"),\n            weight_primary=primary_cfg.get(\"weight\", 0.5),\n            weight_secondary=secondary_cfg.get(\"weight\", 0.3),\n            weight_tertiary=models.get(\"tertiary\", {}).get(\"weight\", 0.2),\n            reasoning=phase_config.get(\"reasoning\", \"\")\n        )\n    \n    def get_cost_limits(self) -> Dict[str, float]:\n        \"\"\"\n        Get cost management configuration.\n        \n        Returns\n        -------\n        dict\n            Cost budgets and thresholds.\n        \"\"\"\n        return self.config.get(\"cost_management\", {}).get(\"budgets\", {})\n    \n    def should_enable_cache(self) -> bool:\n        \"\"\"Check if caching is enabled in configuration.\"\"\"\n        return self.config.get(\"cost_management\", {}).get(\"optimization\", {}).get(\"enable_cache\", True)\n    \n    def get_cache_ttl(self) -> int:\n        \"\"\"Get cache TTL in seconds.\"\"\"\n        return self.config.get(\"cost_management\", {}).get(\"optimization\", {}).get(\"cache_ttl_seconds\", 3600)\n