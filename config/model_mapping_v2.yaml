version: "3.0"
updated: "2026-02-14"

routing:
  phases:
    gate:
      model: "gpt-5-nano"
      provider: "openai"
      max_tokens: 500
    
    analyst:
      model: "gpt-5-mini"
      provider: "openai"
      max_tokens: 4000
      cascade: ["claude-sonnet-4.5"]
    
    architect:
      model: "gpt-5.2"
      provider: "openai"
      max_tokens: 8000
      consensus_mode: true
      secondary: ["claude-opus-4.6"]
    
    implementer_frontend:
      model: "gpt-5-mini"
      provider: "openai"
      max_tokens: 16000
      cascade: ["claude-opus-4.6"]
    
    implementer_backend:
      model: "gpt-5-mini"
      provider: "openai"
      max_tokens: 16000
      cascade: ["claude-opus-4.6"]
    
    tester:
      model: "gpt-5-mini"
      provider: "openai"
      max_tokens: 8000
    
    reviewer:
      model: "gpt-5-mini"
      provider: "openai"
      max_tokens: 4000
      cascade: ["gpt-5.2"]
    
    self_healer:
      model: "claude-opus-4.6"
      provider: "anthropic"
      max_tokens: 8000
    
    research:
      model: "sonar-deep-research"
      provider: "perplexity"
      fallback: ["gemini-3-pro"]
    
    monitor:
      model: "gemini-3-flash"
      provider: "google"
      max_tokens: 2000
    
    fact_checker:
      model: "sonar"
      provider: "perplexity"

  specialties:
    dotnet_api:
      model: "gpt-5.2"
      provider: "openai"
    efcore:
      model: "gpt-5.2"
      provider: "openai"
    react:
      model: "gpt-5-mini"
      provider: "openai"
    typescript:
      model: "gpt-5-mini"
      provider: "openai"
    documentation:
      model: "gemini-3-pro"
      provider: "google"
      max_tokens: 32000

caching:
  enabled: true
  tier_1_rules: { ttl_seconds: 3600 }
  tier_2_tokens: { ttl_seconds: 3600 }
  tier_3_catalog: { ttl_seconds: 1800 }

cost_management:
  budget_alert_threshold: 0.80
  max_cascade_depth: 2
  prefer_cached: true
