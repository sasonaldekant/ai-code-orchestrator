```yaml
version: "2.0.0"

providers:
  openai:
    api_key: ${OPENAI_API_KEY}
    models:
      - gpt-4o
      - gpt-4o-mini
      - o1 # High reasoning for Architect phase
      - o1-mini
  
  anthropic:
    api_key: ${ANTHROPIC_API_KEY}
    models:
      - claude-3-7-sonnet # Best for Implementation/Self-Healing
      - claude-3-5-sonnet-latest
      - claude-3-5-haiku # Cost-effective for simple Monitoring
  
  google:
    api_key: ${GOOGLE_API_KEY}
    models:
      - gemini-2.0-flash # Fastest for Monitoring logs (Tier 0)
      - gemini-2.0-pro-exp
      - gemini-1.5-pro # Best for large context research
 
  perplexity:
    api_key: ${PERPLEXITY_API_KEY}
    models:
      - sonar-reasoning-pro
      - sonar-reasoning
      - sonar-pro
      - sonar # Base model for fact checking ($1/1M)

default:
  model: gpt-4o-mini
  temperature: 0.0
  max_tokens: 4000
  timeout_seconds: 120

routing:
  phase:
    gate:
      model: gemini-2.0-flash
      provider: google
      temperature: 0.0
      max_tokens: 1000
      tier: 0
      reasoning: "Cheapest model for prompt validation and classification"

    fact_checker:
      model: sonar
      provider: perplexity
      temperature: 0.0
      max_tokens: 2000
      tier: 0
      reasoning: "Real-time verification of libraries and APIs"

    research:
      model: sonar-reasoning-pro
      provider: perplexity
      temperature: 0.1
      max_tokens: 8000
      tier: 3
      cascade:
        - { model: sonar, provider: perplexity }
        - { model: gemini-1.5-pro, provider: google, condition: "context > 100k" }
      reasoning: "Deep web research and large context analysis"

    analyst:
      model: gpt-4o-mini
      provider: openai
      temperature: 0.1
      max_tokens: 4000
      tier: 1
      cascade:
        - { model: claude-3-7-sonnet, provider: anthropic, threshold: 0.8 }
      reasoning: "Small model first, escalate to Sonnet if low confidence"
    
    architect:
      model: gpt-4o
      provider: openai
      temperature: 0.1
      max_tokens: 8000
      tier: 2
      consensus_mode: true
      cascade:
        - { model: claude-3-7-sonnet, provider: anthropic, role: "secondary" }
      reasoning: "Architecture design with consensus for stability"
    
    implementer:
      model: gpt-4o-mini
      provider: openai
      temperature: 0.0
      max_tokens: 8000
      tier: 1
      cascade:
        - { model: claude-3-7-sonnet, provider: anthropic, on_failure: true }
      reasoning: "Strong C# performance, auto-escalate on build fail"
    
    tester:
      model: gpt-4o-mini
      provider: openai
      temperature: 0.0
      max_tokens: 4000
      tier: 1
      reasoning: "Cost-effective for test generation"
    
    reviewer:
      model: gpt-4o-mini
      provider: openai
      temperature: 0.0
      max_tokens: 4000
      tier: 1
      cascade:
        - { model: gpt-4o, provider: openai, condition: "complexity == 'high'" }
      producer_reviewer_loop: true
      max_iterations: 3
      quality_threshold: 8.0
      reasoning: "Checklist review first, deep review only if needed"
    
    monitor:
      model: gemini-2.0-flash
      provider: google
      temperature: 0.0
      max_tokens: 2000
      tier: 0
      reasoning: "Lowest cost for log parsing"

    self_healer:
      model: claude-3-7-sonnet
      provider: anthropic
      temperature: 0.0
      max_tokens: 8000
      tier: 3
      reasoning: "Best-in-class coding logic for resolving complex build errors"

  specialty:
    backend:
      dotnet_api:
        model: gpt-4o
        provider: openai
        temperature: 0.0
        specialization: "ASP.NET Core, Web API, Controllers"
      
      efcore:
        model: gpt-4o
        provider: openai
        temperature: 0.0
        context_aware: true
        context_sources:
          - existing_dbcontext
          - migration_history
        specialization: "Entity Framework Core, Migrations, DbContext"
      
      database:
        model: gpt-4o
        provider: openai
        temperature: 0.0
        specialization: "Schema design, relationships, indexing"
      
      microservice:
        model: gpt-4o
        provider: openai
        temperature: 0.0
        specialization: "Microservice architecture, communication patterns"
      
      security:
        model: gpt-4o
        provider: openai
        temperature: 0.0
        specialization: "Security best practices, vulnerability detection"
    
    frontend:
      react:
        model: gpt-4o-mini
        provider: openai
        temperature: 0.0
        specialization: "React 18+, hooks, functional components"
      
      typescript:
        model: gpt-4o-mini
        provider: openai
        temperature: 0.0
        specialization: "TypeScript types, interfaces, generics"
      
      css:
        model: gpt-4o-mini
        provider: openai
        temperature: 0.0
        specialization: "CSS, SCSS, responsive design"
      
      ui_ux:
        model: gpt-4o-mini
        provider: openai
        temperature: 0.1
        specialization: "User experience, accessibility"
    
    integration:
      api_integration:
        model: gpt-4o
        provider: openai
        temperature: 0.0
        specialization: "REST APIs, HTTP clients, error handling"
      
      devops:
        model: gpt-4o
        provider: openai
        temperature: 0.0
        specialization: "CI/CD, Docker, Kubernetes"
    
    documentation:
      technical_docs:
        model: gemini-2.5-pro
        provider: google
        temperature: 0.2
        max_tokens: 32000
        specialization: "Technical documentation, API docs"
      
      code_research:
        model: sonar-reasoning-pro
        provider: perplexity
        temperature: 0.1
        max_tokens: 8000
        specialization: "Up-to-date research, library versions, and community best practices"


cost_management:
  budgets:
    per_task: 0.50      # USD
    per_hour: 5.00
    per_day: 40.00
    per_month: 800.00
  
  alerts:
    threshold_warning: 0.75   # 75% of budget
    threshold_critical: 0.90  # 90% of budget
    notify_channels:
      - slack_webhook
      - email
  
  optimization:
    enable_cache: true
    cache_ttl_seconds: 3600
    enable_token_estimation: true
    reject_over_budget: true



