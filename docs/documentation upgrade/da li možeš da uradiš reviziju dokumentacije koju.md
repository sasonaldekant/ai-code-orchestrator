<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# da li moÅ¾eÅ¡ da uradiÅ¡ reviziju dokumentacije koju imamo u projektu i dopuniÅ¡ je svime povime , a onda napraviÅ¡ plan detaljan implementacije svega Å¡to nedostaje i projektu

OdliÄo! Sada Ä‡u kreirati sveobuhvatne dokumente. PoÅ¡to ne mogu direktno da kreiram fajlove na GitHub-u putem web interfejsa, pripremiÄ‡iu kompletne dokumente koje moÅ¾eÅ¡ da kopirate i dodaÅ¡ u projekat.

# ðŸ“„ DOKUMENT 1: Revidirani TehniÄki Dokument

KreiraÄ‡u fajl: `docs/AI-Code-Orchestrator-Enhanced-Documentation.md`

```markdown
# AI Code Orchestrator â€” Enhanced Technical Documentation
**Version:** 2.0.0  
**Last Updated:** 2026-02-07  
**Status:** Production-Ready Roadmap

---

## ðŸ“‹ Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [Architecture Overview](#2-architecture-overview)
3. [Multi-Model Orchestration Strategy](#3-multi-model-orchestration-strategy)
4. [Token Optimization Techniques](#4-token-optimization-techniques)
5. [Agent Communication Protocols](#5-agent-communication-protocols)
6. [Quality Assurance Mechanisms](#6-quality-assurance-mechanisms)
7. [.NET-Specific Implementations](#7-net-specific-implementations)
8. [Cost Management & Monitoring](#8-cost-management--monitoring)
9. [Implementation Phases](#9-implementation-phases)
10. [API Reference](#10-api-reference)

---

## 1. Executive Summary

### 1.1 Current State (v0.1.0)
AI Code Orchestrator je scaffold sistem za mikrospecijalizovanu agentsku orkestraciju sa sledeÄ‡im komponentama:
- Phase agents (analyst, architect, implementer, tester)
- Specialist agents (frontend: CSS/TS/React; backend: .NET; integration; DevOps)
- Schema-driven outputs (JSON Schema validacija)
- RAG layer (embeddings + ingest/query)
- REST API (FastAPI) sa auth/rate limiting/metrics
- Tracing/Audit (JSONL)

**Identifikovani problemi:**
- âŒ Mono-vendor pristup (samo OpenAI modeli)
- âŒ Nema token optimizacionih tehnika (visoka potroÅ¡nja)
- âŒ Nedostaje meÄ‘usobna kontrola agenata
- âŒ Nema cost trackinga
- âŒ Sekvencijalno izvrÅ¡avanje (nema paralelizacije)
- âŒ Nedostaju .NET-specifiÄni agenti za existing codebase

### 1.2 Target State (v2.0.0)
**Ciljevi:**
- âœ… Multi-model routing (OpenAI + Claude + Gemini)
- âœ… 60-87% token reduction kroz optimizacione tehnike
- âœ… Producer-Reviewer loop za kvalitet
- âœ… Real-time cost tracking i budget management
- âœ… Parallel execution (MapReduce pattern)
- âœ… EF Core aware agents za incremental development
- âœ… ROI: $1,950 uÅ¡tede po feature-u (81%)

---

## 2. Architecture Overview

### 2.1 Komponente Sistema

```

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    API GATEWAY (FastAPI)                     â”‚
â”‚  - Authentication (JWT + API Key)                            â”‚
â”‚  - Rate Limiting (Redis-based)                               â”‚
â”‚  - Request Routing                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ORCHESTRATOR   â”‚              â”‚  COST MANAGER        â”‚
â”‚  - Multi-model  â”‚              â”‚  - Token tracking    â”‚
â”‚  - Parallel execâ”‚              â”‚  - Budget control    â”‚
â”‚  - Context mgmt â”‚              â”‚  - Real-time alerts  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          â”‚          â”‚          â”‚          â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”
â”‚Router â”‚  â”‚RAG   â”‚  â”‚Trace  â”‚  â”‚Valid. â”‚  â”‚Queue â”‚
â”‚       â”‚  â”‚      â”‚  â”‚       â”‚  â”‚       â”‚  â”‚      â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”‚ Phase/Specialty-based routing
â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AGENT LAYER                            â”‚
â”‚                                                            â”‚
â”‚  Phase Agents:                                            â”‚
â”‚  â”œâ”€ Analyst       (Claude Sonnet 3.5)                    â”‚
â”‚  â”œâ”€ Architect     (Claude Sonnet 3.5 + Consensus)        â”‚
â”‚  â”œâ”€ Implementer   (GPT-4o / Specialized per tech)        â”‚
â”‚  â””â”€ Tester        (GPT-4o mini)                          â”‚
â”‚                                                            â”‚
â”‚  Specialist Agents:                                       â”‚
â”‚  â”œâ”€ Backend                                               â”‚
â”‚  â”‚   â”œâ”€ .NET API Dev        (GPT-4o)                     â”‚
â”‚  â”‚   â”œâ”€ EF Core Manager     (GPT-4o + AST parsing)       â”‚
â”‚  â”‚   â”œâ”€ Database Designer   (Claude Sonnet)              â”‚
â”‚  â”‚   â””â”€ Security Specialist (Claude Sonnet)              â”‚
â”‚  â”œâ”€ Frontend                                              â”‚
â”‚  â”‚   â”œâ”€ React Builder       (GPT-4o mini)                â”‚
â”‚  â”‚   â”œâ”€ TypeScript Dev      (GPT-4o mini)                â”‚
â”‚  â”‚   â””â”€ CSS/UX Designer     (GPT-4o mini)                â”‚
â”‚  â”œâ”€ Integration                                           â”‚
â”‚  â”‚   â”œâ”€ API Integrator      (GPT-4o)                     â”‚
â”‚  â”‚   â””â”€ DevOps Engineer     (GPT-4o)                     â”‚
â”‚  â””â”€ Quality Assurance                                     â”‚
â”‚      â”œâ”€ Code Reviewer       (Claude Sonnet 3.5)          â”‚
â”‚      â”œâ”€ Test Generator      (GPT-4o mini)                â”‚
â”‚      â””â”€ Documentation       (Gemini 2.5 Pro)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

### 2.2 Data Flow

```

1. User Request â†’ API Gateway
2. Auth \& Rate Limit Check
3. Cost Budget Check
4. Orchestrator: Select Execution Pattern
â”œâ”€ Sequential (new feature, complex dependencies)
â”œâ”€ Parallel (independent backend/frontend work)
â””â”€ Producer-Reviewer Loop (critical code)
5. Agent Execution
â”œâ”€ Context Retrieval (RAG)
â”œâ”€ Model Selection (Router)
â”œâ”€ Execution (LLM Call)
â””â”€ Validation (Schema + Custom)
6. Inter-Agent Communication (Codified Protocol)
7. Output Aggregation
8. Cost Tracking \& Logging
9. Response to User
```

---

## 3. Multi-Model Orchestration Strategy

### 3.1 Model Selection Matrix

| **Task Type**          | **Primary Model**       | **Fallback**        | **Reason**                           | **Cost/1M Tokens** |
|------------------------|-------------------------|---------------------|--------------------------------------|--------------------|
| **Planning/Analysis**  | Claude Sonnet 3.5       | GPT-4o              | Best reasoning, context understanding| $3.00 in / $15 out |
| **Architecture Design**| Claude Sonnet 3.5       | GPT-4o              | Multi-step planning, consistency     | $3.00 in / $15 out |
| **Backend (.NET/C#)**  | GPT-4o                  | Claude Sonnet       | Strong C# performance (92% HumanEval)| $2.50 in / $10 out |
| **Frontend (React/TS)**| GPT-4o mini             | GPT-4o              | Cost-effective, good TypeScript      | $0.15 in / $0.60 out|
| **Code Review**        | Claude Sonnet 3.5       | GPT-4o              | Multi-perspective analysis           | $3.00 in / $15 out |
| **Testing**            | GPT-4o mini             | GPT-4o              | Simple task, cost-effective          | $0.15 in / $0.60 out|
| **Documentation**      | Gemini 2.5 Pro          | GPT-4o              | 1M context window, cheap             | $1.25 in / $5 out  |
| **Research**           | Gemini 2.5 Pro          | Claude Sonnet       | Large context for existing code      | $1.25 in / $5 out  |

### 3.2 Routing Configuration

**Fajl:** `config/model_mapping_enhanced.yaml`

```yaml
version: "2.0"
default:
  model: gpt-4o-mini
  temperature: 0.0
  max_tokens: 4000
  timeout: 30

routing:
  phase:
    analyst:
      model: claude-3-5-sonnet
      temperature: 0.1
      max_tokens: 8000
      provider: anthropic
    
    architect:
      model: claude-3-5-sonnet
      temperature: 0.1
      max_tokens: 16000
      provider: anthropic
      consensus_mode: true  # Use 3 models for consensus
      consensus_models:
        - claude-3-5-sonnet
        - gpt-4o
        - gemini-2.5-pro
    
    implementer:
      model: gpt-4o
      temperature: 0.0
      max_tokens: 8000
      provider: openai
    
    tester:
      model: gpt-4o-mini
      temperature: 0.0
      max_tokens: 4000
      provider: openai

  specialty:
    backend:
      dotnet:
        model: gpt-4o
        temperature: 0.0
        provider: openai
      database:
        model: claude-3-5-sonnet
        temperature: 0.0
        provider: anthropic
      efcore:
        model: gpt-4o
        temperature: 0.0
        provider: openai
        context_aware: true  # Load existing DbContext
    
    frontend:
      react:
        model: gpt-4o-mini
        temperature: 0.0
        provider: openai
      typescript:
        model: gpt-4o-mini
        temperature: 0.0
        provider: openai
      css:
        model: gpt-4o-mini
        temperature: 0.0
        provider: openai
    
    review:
      code:
        model: claude-3-5-sonnet
        temperature: 0.0
        provider: anthropic
      security:
        model: claude-3-5-sonnet
        temperature: 0.0
        provider: anthropic
    
    documentation:
      technical:
        model: gemini-2.5-pro
        temperature: 0.2
        provider: google
        max_tokens: 32000
      research:
        model: gemini-2.5-pro
        temperature: 0.3
        provider: google

cost_limits:
  per_task: 0.50  # USD
  per_hour: 5.00
  per_day: 40.00
  alert_threshold: 0.80  # Alert at 80% of budget

optimization:
  enable_agentic_retrieval: true
  enable_codified_communication: true
  enable_context_truncation: true
  enable_minimal_context_passing: true
  parallel_execution: true
```


### 3.3 Provider Integration

**Fajl:** `core/llm_client_enhanced.py`

```python
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Optional, Dict, Any
import openai
import anthropic
import google.generativeai as genai

@dataclass
class LLMResponse:
    content: str
    model: str
    tokens_used: Dict[str, int]  # {input: X, output: Y}
    cost: float
    latency_ms: int
    provider: str

class LLMProvider(ABC):
    @abstractmethod
    async def generate(self, prompt: str, config: Dict[str, Any]) -> LLMResponse:
        pass

class OpenAIProvider(LLMProvider):
    def __init__(self, api_key: str):
        self.client = openai.AsyncOpenAI(api_key=api_key)
    
    async def generate(self, prompt: str, config: Dict[str, Any]) -> LLMResponse:
        start_time = time.time()
        
        response = await self.client.chat.completions.create(
            model=config["model"],
            messages=[{"role": "user", "content": prompt}],
            temperature=config.get("temperature", 0.0),
            max_tokens=config.get("max_tokens", 4000)
        )
        
        latency = int((time.time() - start_time) * 1000)
        tokens = {
            "input": response.usage.prompt_tokens,
            "output": response.usage.completion_tokens
        }
        cost = self._calculate_cost(config["model"], tokens)
        
        return LLMResponse(
            content=response.choices.message.content,
            model=config["model"],
            tokens_used=tokens,
            cost=cost,
            latency_ms=latency,
            provider="openai"
        )
    
    def _calculate_cost(self, model: str, tokens: Dict[str, int]) -> float:
        # Pricing per 1M tokens
        prices = {
            "gpt-4o": {"input": 2.50, "output": 10.0},
            "gpt-4o-mini": {"input": 0.15, "output": 0.60}
        }
        price = prices.get(model, prices["gpt-4o"])
        return (tokens["input"] / 1_000_000 * price["input"] + 
                tokens["output"] / 1_000_000 * price["output"])

class AnthropicProvider(LLMProvider):
    def __init__(self, api_key: str):
        self.client = anthropic.AsyncAnthropic(api_key=api_key)
    
    async def generate(self, prompt: str, config: Dict[str, Any]) -> LLMResponse:
        start_time = time.time()
        
        response = await self.client.messages.create(
            model=config["model"],
            max_tokens=config.get("max_tokens", 4000),
            temperature=config.get("temperature", 0.0),
            messages=[{"role": "user", "content": prompt}]
        )
        
        latency = int((time.time() - start_time) * 1000)
        tokens = {
            "input": response.usage.input_tokens,
            "output": response.usage.output_tokens
        }
        cost = self._calculate_cost(config["model"], tokens)
        
        return LLMResponse(
            content=response.content.text,
            model=config```

